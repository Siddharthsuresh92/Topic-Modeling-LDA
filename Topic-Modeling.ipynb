{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/siddharth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd # to work with dataframes\n",
    "from wordcloud import WordCloud # to analyze frequency of different words in the corpus\n",
    "import re # for using regular expressions\n",
    "\n",
    "# used for pre-processing the text data and unsupervised topic modeling\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# used for natural language processing {NLTK: Natural Language Tool-Kit} \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed to reproduce the results later. Seed used here is '2019'\n",
    "import numpy as np\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of articles: 1103663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>committal continues into goulburn jail riot</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>costello unhappy he wasnt consulted by stone</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>council approves poultry farm</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>council awaits more rain</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>council considers indigenous caravan park plan</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>council elections planned for may</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>council rejects combined field days stand idea</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>council to change tree protection by law</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>council to fund groundwater study</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>counsel begin summing up at warnes doping hearing</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>criminal charges pending in south korea subway probe</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>crocs prove too good for bullets</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>date set for bushfires coronial inquiry</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>dean to receive lifetime parliamentary pension</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>death spells end to record marriage</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>demons thump tigers</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>deportivo cali outclass sulky river plate</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>deschamps chases old flame marseille</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>disaster funds to allow shire to complete other</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>dortmund ready to hound bayern</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text  index\n",
       "500  committal continues into goulburn jail riot           500  \n",
       "501  costello unhappy he wasnt consulted by stone          501  \n",
       "502  council approves poultry farm                         502  \n",
       "503  council awaits more rain                              503  \n",
       "504  council considers indigenous caravan park plan        504  \n",
       "505  council elections planned for may                     505  \n",
       "506  council rejects combined field days stand idea        506  \n",
       "507  council to change tree protection by law              507  \n",
       "508  council to fund groundwater study                     508  \n",
       "509  counsel begin summing up at warnes doping hearing     509  \n",
       "510  criminal charges pending in south korea subway probe  510  \n",
       "511  crocs prove too good for bullets                      511  \n",
       "512  date set for bushfires coronial inquiry               512  \n",
       "513  dean to receive lifetime parliamentary pension        513  \n",
       "514  death spells end to record marriage                   514  \n",
       "515  demons thump tigers                                   515  \n",
       "516  deportivo cali outclass sulky river plate             516  \n",
       "517  deschamps chases old flame marseille                  517  \n",
       "518  disaster funds to allow shire to complete other       518  \n",
       "519  dortmund ready to hound bayern                        519  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "news = pd.read_csv(\"abcnews-date-text.csv\")\n",
    "news = news.drop(columns = \"publish_date\") # since this data isn't really required for the topic modeling\n",
    "news['index'] = news.index # we would want to give each article an index to reference back to it later\n",
    "print(\"total number of articles: \" + str(len(news)))\n",
    "news[500:520]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just from eyeballing this section of the data (rows 500 to 519), it can be observed that there are articles from politics, law, sports just to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Pre-processing includes the following steps:\n",
    "\n",
    "1. <font color=\"blue\"> Stemming-lemmatization </font>: This step is required to extract the rootwords from a document. For instance, the root word for happiness is \"happy\"\n",
    "2. <font color=\"blue\"> Removing stopwords </font>: Stopwords are the most commonly used words in natural language. Examples: \"The\", \"a\", \"is\", \"at\", \"which\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stemming-Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many stemmers in the NLTK library.\n",
    "\n",
    "Some of the frequently used stemmers are:\n",
    "1. SnowballStemmer\n",
    "2. LancasterStemmer\n",
    "3. PorterStemmer\n",
    "\n",
    "The lemmatizer in the NLTK library is: WordNetLemmatizer\n",
    "\n",
    "Let's look at how these would work on different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happy</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "      <td>fly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>workers</td>\n",
       "      <td>work</td>\n",
       "      <td>worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dogs</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agree</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "      <td>owned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "      <td>humbled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "      <td>meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>helper</td>\n",
       "      <td>help</td>\n",
       "      <td>helper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>drinks</td>\n",
       "      <td>drink</td>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>watching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "      <td>traditional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>politics</td>\n",
       "      <td>polit</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>player</td>\n",
       "      <td>play</td>\n",
       "      <td>player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>curator</td>\n",
       "      <td>cur</td>\n",
       "      <td>curator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>better</td>\n",
       "      <td>bet</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed        lemma\n",
       "0   happiness     happy   happiness  \n",
       "1   flies         fli     fly        \n",
       "2   workers       work    worker     \n",
       "3   dogs          dog     dog        \n",
       "4   agreed        agree   agreed     \n",
       "5   owned         own     owned      \n",
       "6   humbled       humbl   humbled    \n",
       "7   meeting       meet    meeting    \n",
       "8   helper        help    helper     \n",
       "9   drinks        drink   drink      \n",
       "10  watching      watch   watching   \n",
       "11  traditional   tradit  traditional\n",
       "12  politics      polit   politics   \n",
       "13  player        play    player     \n",
       "14  curator       cur     curator    \n",
       "15  better        bet     better     \n",
       "16  best          best    best       \n",
       "17  cooker        cook    cooker     \n",
       "18  cooking       cook    cooking    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample words to stem/lemmatize\n",
    "sample_words = ['happiness', 'flies', 'workers', 'dogs', 'agreed', 'owned', 'humbled', 'meeting', 'helper', \n",
    "                'drinks', 'watching', 'traditional', 'politics', 'player', 'curator', 'better', 'best', \n",
    "                'cooker', 'cooking']\n",
    "\n",
    "# initializing the stemmer and lemmatizer\n",
    "stemmer = LancasterStemmer() # using the LancasterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stemming process\n",
    "stems = [stemmer.stem(plural) for plural in sample_words]\n",
    "\n",
    "# lemmatization process\n",
    "lemmas = [lemmatizer.lemmatize(plural) for plural in sample_words]\n",
    "\n",
    "# saves the results in a dictionary and creates a dataframe from it\n",
    "pd.DataFrame(data = {'original word': sample_words, 'stemmed': stems, 'lemma': lemmas})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe above the following observations were made:\n",
    "1. For some words, the lemmatization and stemming provide the exact same result (eg: \"dogs\", \"drinks\")\n",
    "2. Lemmatization does not change certain words (eg: \"agreed\", \"player\", \"watcher\")\n",
    "3. Stemming converts certain words to something that does not seem to be a word anymore (eg: \"better\", \"curator\", \"politics\", \"traditional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color = \"black\"> Q. There is an argument in the WordNetLemmatizer.lemmatize() called \"pos\" which stands for Part-of-speech. Do you think that could make a difference for the lemmatization process for the words given above? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: by default, the \"pos\" argument is equal to \"n\"\n",
    "\n",
    "where, n stands for NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemma-noun</th>\n",
       "      <th>lemma-verb</th>\n",
       "      <th>lemma-adjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happy</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "      <td>fly</td>\n",
       "      <td>fly</td>\n",
       "      <td>flies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>workers</td>\n",
       "      <td>work</td>\n",
       "      <td>worker</td>\n",
       "      <td>workers</td>\n",
       "      <td>workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dogs</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agree</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agree</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "      <td>owned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humble</td>\n",
       "      <td>humbled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "      <td>meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>helper</td>\n",
       "      <td>help</td>\n",
       "      <td>helper</td>\n",
       "      <td>helper</td>\n",
       "      <td>helper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>drinks</td>\n",
       "      <td>drink</td>\n",
       "      <td>drink</td>\n",
       "      <td>drink</td>\n",
       "      <td>drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>watching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "      <td>traditional</td>\n",
       "      <td>traditional</td>\n",
       "      <td>traditional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>politics</td>\n",
       "      <td>polit</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>player</td>\n",
       "      <td>play</td>\n",
       "      <td>player</td>\n",
       "      <td>player</td>\n",
       "      <td>player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>curator</td>\n",
       "      <td>cur</td>\n",
       "      <td>curator</td>\n",
       "      <td>curator</td>\n",
       "      <td>curator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>better</td>\n",
       "      <td>bet</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cooker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed   lemma-noun   lemma-verb lemma-adjective\n",
       "0   happiness     happy   happiness    happiness    happiness     \n",
       "1   flies         fli     fly          fly          flies         \n",
       "2   workers       work    worker       workers      workers       \n",
       "3   dogs          dog     dog          dog          dogs          \n",
       "4   agreed        agree   agreed       agree        agreed        \n",
       "5   owned         own     owned        own          owned         \n",
       "6   humbled       humbl   humbled      humble       humbled       \n",
       "7   meeting       meet    meeting      meet         meeting       \n",
       "8   helper        help    helper       helper       helper        \n",
       "9   drinks        drink   drink        drink        drinks        \n",
       "10  watching      watch   watching     watch        watching      \n",
       "11  traditional   tradit  traditional  traditional  traditional   \n",
       "12  politics      polit   politics     politics     politics      \n",
       "13  player        play    player       player       player        \n",
       "14  curator       cur     curator      curator      curator       \n",
       "15  better        bet     better       better       good          \n",
       "16  best          best    best         best         best          \n",
       "17  cooker        cook    cooker       cooker       cooker        \n",
       "18  cooking       cook    cooking      cook         cooking       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization process (pos = \"verb\")\n",
    "lemma_v = [lemmatizer.lemmatize(plural, pos = \"v\") for plural in sample_words]\n",
    "\n",
    "# lemmatization process (pos = \"verb\")\n",
    "lemma_a = [lemmatizer.lemmatize(plural, pos = \"a\") for plural in sample_words]\n",
    "\n",
    "# saves the results in a dictionary and creates a dataframe from it\n",
    "pd.DataFrame(data = {'original word': sample_words, 'stemmed': stems, 'lemma-noun': lemmas, 'lemma-verb': lemma_v,\n",
    "                    'lemma-adjective': lemma_a})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which part-of-speech argument should be used for this problem?\n",
    "\n",
    "For this case, since we are modeling topics for a corpus of news articles, using verbs for the part-of-speech argument does seem to be intuitive. This is because we want to associate the articles to different topics based on the words we observe in a cluster of documents.\n",
    "\n",
    "For instance, if we observe the words \"cook\", \"chef\", \"vegetables\", \"restaurant\", \"soup\", \"wine\" as the most commonly occuring words, we would want to choose a topic such as say \"CULINARY NEWS\" in the context of news articles.\n",
    "\n",
    "So, in order to get the right root, we might want to extract the root based on the part of speech being set to \"verb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way could be to stem the resultant lemmatized word. You will see a lot of people use a combination of these methods to extract the roots, but for this analysis, I shall be using the lemmas only. You can see the result below when lemmatization was used with pos = \"v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemma-verb</th>\n",
       "      <th>stem-lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happy</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "      <td>fly</td>\n",
       "      <td>fly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>workers</td>\n",
       "      <td>work</td>\n",
       "      <td>workers</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dogs</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agree</td>\n",
       "      <td>agree</td>\n",
       "      <td>agr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "      <td>own</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "      <td>humble</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "      <td>meet</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>helper</td>\n",
       "      <td>help</td>\n",
       "      <td>helper</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>drinks</td>\n",
       "      <td>drink</td>\n",
       "      <td>drink</td>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>watch</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>politics</td>\n",
       "      <td>polit</td>\n",
       "      <td>politics</td>\n",
       "      <td>polit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>player</td>\n",
       "      <td>play</td>\n",
       "      <td>player</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>curator</td>\n",
       "      <td>cur</td>\n",
       "      <td>curator</td>\n",
       "      <td>cur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>better</td>\n",
       "      <td>bet</td>\n",
       "      <td>better</td>\n",
       "      <td>bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cook</td>\n",
       "      <td>cooker</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>cook</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed   lemma-verb stem-lemma\n",
       "0   happiness     happy   happiness    happy    \n",
       "1   flies         fli     fly          fly      \n",
       "2   workers       work    workers      work     \n",
       "3   dogs          dog     dog          dog      \n",
       "4   agreed        agree   agree        agr      \n",
       "5   owned         own     own          own      \n",
       "6   humbled       humbl   humble       humbl    \n",
       "7   meeting       meet    meet         meet     \n",
       "8   helper        help    helper       help     \n",
       "9   drinks        drink   drink        drink    \n",
       "10  watching      watch   watch        watch    \n",
       "11  traditional   tradit  traditional  tradit   \n",
       "12  politics      polit   politics     polit    \n",
       "13  player        play    player       play     \n",
       "14  curator       cur     curator      cur      \n",
       "15  better        bet     better       bet      \n",
       "16  best          best    best         best     \n",
       "17  cooker        cook    cooker       cook     \n",
       "18  cooking       cook    cook         cook     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined lemmatization and stemming process (in that order)\n",
    "stem_lemma = [stemmer.stem(WordNetLemmatizer().lemmatize(plural, pos = \"v\")) for plural in sample_words]\n",
    "\n",
    "pd.DataFrame(data = {'original word': sample_words, 'stemmed': stems, 'lemma-verb': lemma_v,\n",
    "                    'stem-lemma': stem_lemma})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataframe above, I would prefer the lemma-verb column more than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a function that lemmatizes given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize a given word\n",
    "\n",
    "def lemmatize(text):\n",
    "    return lemmatizer.lemmatize(text, pos = \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords can be removed from text by comparing individual words in a sentence with a defined list of STOPWORDS. The STOPWORDS from gensim.parsing.preprocessing library has a list of such stopwords in the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'computer',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'didn',\n",
       "           'do',\n",
       "           'does',\n",
       "           'doesn',\n",
       "           'doing',\n",
       "           'don',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'just',\n",
       "           'keep',\n",
       "           'kg',\n",
       "           'km',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'make',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'quite',\n",
       "           'rather',\n",
       "           're',\n",
       "           'really',\n",
       "           'regarding',\n",
       "           'same',\n",
       "           'say',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'unless',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'used',\n",
       "           'using',\n",
       "           'various',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords in the list\n",
    "gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how it works for a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"We are trying to learn how to implement topic modeling using LDA\"\n",
    "sentence2 = \"This is a sample sentence to check how to remove stopwords from a sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall tokenize the above sentences and then compare it with the stopwords. In order to do that, we will use the gensim.utils.simple_preprocess() function. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'trying', 'to', 'learn', 'how', 'to', 'implement', 'topic', 'modeling', 'using', 'lda']\n",
      "['this', 'is', 'sample', 'sentence', 'to', 'check', 'how', 'to', 'remove', 'stopwords', 'from', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "print(gensim.utils.simple_preprocess(sentence1))\n",
    "print(gensim.utils.simple_preprocess(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization works, so next let's try to remove all the stopwords from each of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trying', 'learn', 'implement', 'topic', 'modeling']\n",
      "['sample', 'sentence', 'check', 'remove', 'stopwords', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# initializing two empty lists for each of the sentences\n",
    "res1 = []\n",
    "res2 = []\n",
    "\n",
    "for word in gensim.utils.simple_preprocess(sentence1):\n",
    "    if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:\n",
    "        res1.append(word)\n",
    "        \n",
    "for word in gensim.utils.simple_preprocess(sentence2):\n",
    "    if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:\n",
    "        res2.append(word)\n",
    "\n",
    "print(res1)\n",
    "print(res2)\n",
    "\n",
    "# Note that we also checked for the length of the words along with removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to remove the stopwords, we can combine the two processes of stemming/lemmatization and removing stopwords and define a function to help us do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the text\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for word in gensim.utils.simple_preprocess(text):\n",
    "        if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:\n",
    "            l = lemmatize(word)\n",
    "            if len(l) > 3:\n",
    "                result.append(l)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works for the news article at index = 500 from the data frame above that had the headline \"<font color = \"red\">committal continues into goulburn jail riot</font>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['committal', 'continue', 'goulburn', 'jail', 'riot']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(news.iloc[500,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!!! Now we can implement this preprocessing function over the entire data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "processed_news = news['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500    [committal, continue, goulburn, jail, riot]             \n",
       "501    [costello, unhappy, wasnt, consult, stone]              \n",
       "502    [council, approve, poultry, farm]                       \n",
       "503    [council, await, rain]                                  \n",
       "504    [council, consider, indigenous, caravan, park, plan]    \n",
       "505    [council, elections, plan]                              \n",
       "506    [council, reject, combine, field, days, stand, idea]    \n",
       "507    [council, change, tree, protection]                     \n",
       "508    [council, fund, groundwater, study]                     \n",
       "509    [counsel, begin, warn, dope, hear]                      \n",
       "510    [criminal, charge, pending, south, korea, subway, probe]\n",
       "511    [crocs, prove, good, bullets]                           \n",
       "512    [date, bushfires, coronial, inquiry]                    \n",
       "513    [dean, receive, lifetime, parliamentary, pension]       \n",
       "514    [death, spell, record, marriage]                        \n",
       "515    [demons, thump, tigers]                                 \n",
       "516    [deportivo, cali, outclass, sulky, river, plate]        \n",
       "517    [deschamps, chase, flame, marseille]                    \n",
       "518    [disaster, fund, allow, shire, complete]                \n",
       "519    [dortmund, ready, hound, bayern]                        \n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_news[500:520]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to model, we will try to examine these approaches:\n",
    "1. LDA using bag of words\n",
    "2. LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses frequency of occurence of words in the corpus as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creates a dictionary of unique words from the corpus\n",
    "dictionary = Dictionary(processed_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 community\n",
      "2 decide\n",
      "3 licence\n",
      "4 aware\n",
      "5 defamation\n",
      "6 witness\n",
      "7 call\n",
      "8 infrastructure\n",
      "9 protection\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77409"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the number of unique words identified from the pre-processed news articles\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are about 1.1 million articles in the dataset, the number of unique words that would be used for modeling still seem to be high. In order for us to filter this out further, we will try to remove the following type of words:\n",
    "1. All of the words that appear in less than 'x' number of the articles\n",
    "2. All of the words that appear in more than 'y%' of the articles\n",
    "\n",
    "In order to select the values of x and y, we will inspect our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>36127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>22403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>19267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>16922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>16727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50383</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77409 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number of documents\n",
       "238    36127              \n",
       "47     22403              \n",
       "323    19267              \n",
       "165    16922              \n",
       "213    16727              \n",
       "...      ...              \n",
       "50384  1                  \n",
       "50383  1                  \n",
       "50382  1                  \n",
       "50380  1                  \n",
       "77407  1                  \n",
       "\n",
       "[77409 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document frequencies\n",
    "\n",
    "words_in_articles = pd.DataFrame.from_dict(dictionary.dfs, orient = 'index', columns = [\"number of documents\"])\n",
    "words_in_articles.sort_values(\"number of documents\", ascending = False, inplace = True)\n",
    "words_in_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = words_in_articles.index\n",
    "\n",
    "words_freq = {}\n",
    "for i in list1:\n",
    "    words_freq[dictionary[i]] = words_in_articles['number of documents'][i]\n",
    "\n",
    "words_in_articles['word'] = words_freq.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of documents</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>36127</td>\n",
       "      <td>police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>22403</td>\n",
       "      <td>plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>19267</td>\n",
       "      <td>charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>16922</td>\n",
       "      <td>govt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>16727</td>\n",
       "      <td>court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50384</td>\n",
       "      <td>1</td>\n",
       "      <td>hallahan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50383</td>\n",
       "      <td>1</td>\n",
       "      <td>dazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50382</td>\n",
       "      <td>1</td>\n",
       "      <td>blakefield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50380</td>\n",
       "      <td>1</td>\n",
       "      <td>blechynden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77407</td>\n",
       "      <td>1</td>\n",
       "      <td>ausus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number of documents        word\n",
       "238    36127                police    \n",
       "47     22403                plan      \n",
       "323    19267                charge    \n",
       "165    16922                govt      \n",
       "213    16727                court     \n",
       "...      ...                  ...     \n",
       "50384  1                    hallahan  \n",
       "50383  1                    dazy      \n",
       "50382  1                    blakefield\n",
       "50380  1                    blechynden\n",
       "77407  1                    ausus     \n",
       "\n",
       "[77409 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_extremes(no_below = 100, no_above = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77409"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(headline for headline in news[\"headline_text\"])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(word for i in range(0, len(processed_news)) for word in processed_news[i])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = [dictionary.doc2bow(news) for news in processed_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = models.LdaMulticore(bag_of_words, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.023*\"attack\" + 0.023*\"world\" + 0.022*\"kill\" + 0.020*\"coast\" + 0.017*\"interview\" + 0.014*\"south\" + 0.013*\"tasmania\" + 0.013*\"gold\" + 0.012*\"women\" + 0.009*\"australian\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.019*\"rural\" + 0.015*\"minister\" + 0.014*\"call\" + 0.014*\"lose\" + 0.013*\"need\" + 0.013*\"bank\" + 0.012*\"force\" + 0.011*\"royal\" + 0.011*\"government\" + 0.010*\"hobart\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.031*\"charge\" + 0.029*\"court\" + 0.021*\"murder\" + 0.019*\"face\" + 0.019*\"perth\" + 0.016*\"jail\" + 0.014*\"accuse\" + 0.014*\"home\" + 0.014*\"test\" + 0.013*\"australian\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.019*\"open\" + 0.019*\"country\" + 0.017*\"years\" + 0.015*\"government\" + 0.015*\"power\" + 0.014*\"hour\" + 0.013*\"league\" + 0.013*\"sydney\" + 0.012*\"hospital\" + 0.011*\"state\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.019*\"school\" + 0.016*\"health\" + 0.016*\"fund\" + 0.014*\"help\" + 0.014*\"indigenous\" + 0.012*\"council\" + 0.012*\"turnbull\" + 0.012*\"concern\" + 0.011*\"trial\" + 0.010*\"plan\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.054*\"australia\" + 0.024*\"crash\" + 0.019*\"house\" + 0.018*\"donald\" + 0.017*\"rise\" + 0.015*\"price\" + 0.012*\"children\" + 0.011*\"game\" + 0.011*\"victorian\" + 0.010*\"farmers\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.041*\"trump\" + 0.022*\"market\" + 0.016*\"record\" + 0.015*\"fight\" + 0.015*\"share\" + 0.015*\"break\" + 0.014*\"life\" + 0.014*\"fall\" + 0.010*\"campaign\" + 0.010*\"australias\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.024*\"adelaide\" + 0.019*\"live\" + 0.016*\"tasmanian\" + 0.014*\"family\" + 0.013*\"change\" + 0.012*\"abuse\" + 0.012*\"guilty\" + 0.012*\"victoria\" + 0.011*\"release\" + 0.010*\"find\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.027*\"north\" + 0.019*\"china\" + 0.017*\"west\" + 0.017*\"time\" + 0.013*\"talk\" + 0.013*\"hold\" + 0.012*\"island\" + 0.012*\"john\" + 0.011*\"south\" + 0.011*\"korea\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.062*\"police\" + 0.030*\"queensland\" + 0.028*\"election\" + 0.027*\"melbourne\" + 0.022*\"canberra\" + 0.018*\"miss\" + 0.017*\"shoot\" + 0.013*\"flood\" + 0.012*\"news\" + 0.012*\"search\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "council rejects combined field days stand idea\n",
      "['council', 'reject', 'combine', 'field', 'days', 'stand', 'idea']\n"
     ]
    }
   ],
   "source": [
    "# 505, 506, 507, 508, 509, 510\n",
    "t = bag_of_words[506]\n",
    "show_headline(506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5126144289970398\t \n",
      "Topic: 0.019*\"rural\" + 0.015*\"minister\" + 0.014*\"call\" + 0.014*\"lose\" + 0.013*\"need\" + 0.013*\"bank\" + 0.012*\"force\" + 0.011*\"royal\" + 0.011*\"government\" + 0.010*\"hobart\"\n",
      "\n",
      "\n",
      "Score: 0.2623797059059143\t \n",
      "Topic: 0.024*\"adelaide\" + 0.019*\"live\" + 0.016*\"tasmanian\" + 0.014*\"family\" + 0.013*\"change\" + 0.012*\"abuse\" + 0.012*\"guilty\" + 0.012*\"victoria\" + 0.011*\"release\" + 0.010*\"find\"\n",
      "\n",
      "\n",
      "Score: 0.13747239112854004\t \n",
      "Topic: 0.019*\"open\" + 0.019*\"country\" + 0.017*\"years\" + 0.015*\"government\" + 0.015*\"power\" + 0.014*\"hour\" + 0.013*\"league\" + 0.013*\"sydney\" + 0.012*\"hospital\" + 0.011*\"state\"\n",
      "\n",
      "\n",
      "Score: 0.012507922016084194\t \n",
      "Topic: 0.019*\"school\" + 0.016*\"health\" + 0.016*\"fund\" + 0.014*\"help\" + 0.014*\"indigenous\" + 0.012*\"council\" + 0.012*\"turnbull\" + 0.012*\"concern\" + 0.011*\"trial\" + 0.010*\"plan\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.023*\"attack\" + 0.023*\"world\" + 0.022*\"kill\" + 0.020*\"coast\" + 0.017*\"interview\" + 0.014*\"south\" + 0.013*\"tasmania\" + 0.013*\"gold\" + 0.012*\"women\" + 0.009*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.031*\"charge\" + 0.029*\"court\" + 0.021*\"murder\" + 0.019*\"face\" + 0.019*\"perth\" + 0.016*\"jail\" + 0.014*\"accuse\" + 0.014*\"home\" + 0.014*\"test\" + 0.013*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.054*\"australia\" + 0.024*\"crash\" + 0.019*\"house\" + 0.018*\"donald\" + 0.017*\"rise\" + 0.015*\"price\" + 0.012*\"children\" + 0.011*\"game\" + 0.011*\"victorian\" + 0.010*\"farmers\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.041*\"trump\" + 0.022*\"market\" + 0.016*\"record\" + 0.015*\"fight\" + 0.015*\"share\" + 0.015*\"break\" + 0.014*\"life\" + 0.014*\"fall\" + 0.010*\"campaign\" + 0.010*\"australias\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.027*\"north\" + 0.019*\"china\" + 0.017*\"west\" + 0.017*\"time\" + 0.013*\"talk\" + 0.013*\"hold\" + 0.012*\"island\" + 0.012*\"john\" + 0.011*\"south\" + 0.011*\"korea\"\n",
      "\n",
      "\n",
      "Score: 0.01250425260514021\t \n",
      "Topic: 0.062*\"police\" + 0.030*\"queensland\" + 0.028*\"election\" + 0.027*\"melbourne\" + 0.022*\"canberra\" + 0.018*\"miss\" + 0.017*\"shoot\" + 0.013*\"flood\" + 0.012*\"news\" + 0.012*\"search\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_bow[t], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model_bow.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_headline(n):\n",
    "    print(news.headline_text[n])\n",
    "    print(processed_news[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.38773059844970703\t \n",
      "Topic: 0.054*\"australia\" + 0.024*\"crash\" + 0.019*\"house\" + 0.018*\"donald\" + 0.017*\"rise\" + 0.015*\"price\" + 0.012*\"children\" + 0.011*\"game\" + 0.011*\"victorian\" + 0.010*\"farmers\"\n",
      "\n",
      "\n",
      "Score: 0.262622207403183\t \n",
      "Topic: 0.027*\"north\" + 0.019*\"china\" + 0.017*\"west\" + 0.017*\"time\" + 0.013*\"talk\" + 0.013*\"hold\" + 0.012*\"island\" + 0.012*\"john\" + 0.011*\"south\" + 0.011*\"korea\"\n",
      "\n",
      "\n",
      "Score: 0.13746239244937897\t \n",
      "Topic: 0.031*\"charge\" + 0.029*\"court\" + 0.021*\"murder\" + 0.019*\"face\" + 0.019*\"perth\" + 0.016*\"jail\" + 0.014*\"accuse\" + 0.014*\"home\" + 0.014*\"test\" + 0.013*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.13713382184505463\t \n",
      "Topic: 0.062*\"police\" + 0.030*\"queensland\" + 0.028*\"election\" + 0.027*\"melbourne\" + 0.022*\"canberra\" + 0.018*\"miss\" + 0.017*\"shoot\" + 0.013*\"flood\" + 0.012*\"news\" + 0.012*\"search\"\n",
      "\n",
      "\n",
      "Score: 0.012511197477579117\t \n",
      "Topic: 0.023*\"attack\" + 0.023*\"world\" + 0.022*\"kill\" + 0.020*\"coast\" + 0.017*\"interview\" + 0.014*\"south\" + 0.013*\"tasmania\" + 0.013*\"gold\" + 0.012*\"women\" + 0.009*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.012507959268987179\t \n",
      "Topic: 0.019*\"rural\" + 0.015*\"minister\" + 0.014*\"call\" + 0.014*\"lose\" + 0.013*\"need\" + 0.013*\"bank\" + 0.012*\"force\" + 0.011*\"royal\" + 0.011*\"government\" + 0.010*\"hobart\"\n",
      "\n",
      "\n",
      "Score: 0.012507959268987179\t \n",
      "Topic: 0.019*\"open\" + 0.019*\"country\" + 0.017*\"years\" + 0.015*\"government\" + 0.015*\"power\" + 0.014*\"hour\" + 0.013*\"league\" + 0.013*\"sydney\" + 0.012*\"hospital\" + 0.011*\"state\"\n",
      "\n",
      "\n",
      "Score: 0.012507959268987179\t \n",
      "Topic: 0.019*\"school\" + 0.016*\"health\" + 0.016*\"fund\" + 0.014*\"help\" + 0.014*\"indigenous\" + 0.012*\"council\" + 0.012*\"turnbull\" + 0.012*\"concern\" + 0.011*\"trial\" + 0.010*\"plan\"\n",
      "\n",
      "\n",
      "Score: 0.012507959268987179\t \n",
      "Topic: 0.041*\"trump\" + 0.022*\"market\" + 0.016*\"record\" + 0.015*\"fight\" + 0.015*\"share\" + 0.015*\"break\" + 0.014*\"life\" + 0.014*\"fall\" + 0.010*\"campaign\" + 0.010*\"australias\"\n",
      "\n",
      "\n",
      "Score: 0.012507959268987179\t \n",
      "Topic: 0.024*\"adelaide\" + 0.019*\"live\" + 0.016*\"tasmanian\" + 0.014*\"family\" + 0.013*\"change\" + 0.012*\"abuse\" + 0.012*\"guilty\" + 0.012*\"victoria\" + 0.011*\"release\" + 0.010*\"find\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unseen_news = \"Australia bushfires: New South Wales battles catastrophic conditions\"\n",
    "bow = dictionary.doc2bow(preprocess(unseen_news))\n",
    "\n",
    "for index, score in sorted(lda_model_bow[bow], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model_bow.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.31005433201789856\t \n",
      "Topic: 0.041*\"trump\" + 0.022*\"market\" + 0.016*\"record\" + 0.015*\"fight\" + 0.015*\"share\" + 0.015*\"break\" + 0.014*\"life\" + 0.014*\"fall\" + 0.010*\"campaign\" + 0.010*\"australias\"\n",
      "\n",
      "\n",
      "Score: 0.30996790528297424\t \n",
      "Topic: 0.031*\"charge\" + 0.029*\"court\" + 0.021*\"murder\" + 0.019*\"face\" + 0.019*\"perth\" + 0.016*\"jail\" + 0.014*\"accuse\" + 0.014*\"home\" + 0.014*\"test\" + 0.013*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.11001504212617874\t \n",
      "Topic: 0.019*\"school\" + 0.016*\"health\" + 0.016*\"fund\" + 0.014*\"help\" + 0.014*\"indigenous\" + 0.012*\"council\" + 0.012*\"turnbull\" + 0.012*\"concern\" + 0.011*\"trial\" + 0.010*\"plan\"\n",
      "\n",
      "\n",
      "Score: 0.10999252647161484\t \n",
      "Topic: 0.023*\"attack\" + 0.023*\"world\" + 0.022*\"kill\" + 0.020*\"coast\" + 0.017*\"interview\" + 0.014*\"south\" + 0.013*\"tasmania\" + 0.013*\"gold\" + 0.012*\"women\" + 0.009*\"australian\"\n",
      "\n",
      "\n",
      "Score: 0.1099497377872467\t \n",
      "Topic: 0.019*\"open\" + 0.019*\"country\" + 0.017*\"years\" + 0.015*\"government\" + 0.015*\"power\" + 0.014*\"hour\" + 0.013*\"league\" + 0.013*\"sydney\" + 0.012*\"hospital\" + 0.011*\"state\"\n",
      "\n",
      "\n",
      "Score: 0.010004542768001556\t \n",
      "Topic: 0.062*\"police\" + 0.030*\"queensland\" + 0.028*\"election\" + 0.027*\"melbourne\" + 0.022*\"canberra\" + 0.018*\"miss\" + 0.017*\"shoot\" + 0.013*\"flood\" + 0.012*\"news\" + 0.012*\"search\"\n",
      "\n",
      "\n",
      "Score: 0.010003968141973019\t \n",
      "Topic: 0.019*\"rural\" + 0.015*\"minister\" + 0.014*\"call\" + 0.014*\"lose\" + 0.013*\"need\" + 0.013*\"bank\" + 0.012*\"force\" + 0.011*\"royal\" + 0.011*\"government\" + 0.010*\"hobart\"\n",
      "\n",
      "\n",
      "Score: 0.010003968141973019\t \n",
      "Topic: 0.054*\"australia\" + 0.024*\"crash\" + 0.019*\"house\" + 0.018*\"donald\" + 0.017*\"rise\" + 0.015*\"price\" + 0.012*\"children\" + 0.011*\"game\" + 0.011*\"victorian\" + 0.010*\"farmers\"\n",
      "\n",
      "\n",
      "Score: 0.010003968141973019\t \n",
      "Topic: 0.024*\"adelaide\" + 0.019*\"live\" + 0.016*\"tasmanian\" + 0.014*\"family\" + 0.013*\"change\" + 0.012*\"abuse\" + 0.012*\"guilty\" + 0.012*\"victoria\" + 0.011*\"release\" + 0.010*\"find\"\n",
      "\n",
      "\n",
      "Score: 0.010003968141973019\t \n",
      "Topic: 0.027*\"north\" + 0.019*\"china\" + 0.017*\"west\" + 0.017*\"time\" + 0.013*\"talk\" + 0.013*\"hold\" + 0.012*\"island\" + 0.012*\"john\" + 0.011*\"south\" + 0.011*\"korea\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unseen_news = \"Scientists develop a new method for identifying potentially habitable planets that could host \\\n",
    "                ALIEN LIFE outside of our solar system\"\n",
    "bow = dictionary.doc2bow(preprocess(unseen_news))\n",
    "\n",
    "for index, score in sorted(lda_model_bow[bow], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model_bow.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.read_csv(\"wines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.DataFrame(wines['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal characteristics. Nonetheless, if you think of it as a pleasantly unfussy country wine, it's a good companion to a hearty winter stew.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118835</td>\n",
       "      <td>Notes of honeysuckle and cantaloupe sweeten this deliciously feather-light spätlese. It's intensely juicy, quenching the palate with streams of tart tangerine and grapefruit acidity, yet wraps up with a kiss of honey and peach.</td>\n",
       "      <td>118835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118836</td>\n",
       "      <td>Citation is given as much as a decade of bottle age prior to release, which means it is pre-cellared and drinking at its peak. Baked cherry, cocoa and coconut flavors combine gracefully, with soft, secondary fruit compote highlights.</td>\n",
       "      <td>118836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118837</td>\n",
       "      <td>Well-drained gravel soil gives this wine its crisp and dry character. It is ripe and fruity, although the spice is subdued in favor of a more serious structure. This is a wine to age for a couple of years, so drink from 2017.</td>\n",
       "      <td>118837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118838</td>\n",
       "      <td>A dry style of Pinot Gris, this is crisp with some acidity. It also has weight and a solid, powerful core of spice and baked apple flavors. With its structure still developing, the wine needs to age. Drink from 2015.</td>\n",
       "      <td>118838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118839</td>\n",
       "      <td>Big, rich and off-dry, this is powered by intense spiciness and rounded texture. Lychees dominate the fruit profile, giving an opulent feel to the aftertaste. Drink now.</td>\n",
       "      <td>118839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118840 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                      description  \\\n",
       "0       Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.                                                                                \n",
       "1       This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.                         \n",
       "2       Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.                                                                  \n",
       "3       Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.                                                     \n",
       "4       Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal characteristics. Nonetheless, if you think of it as a pleasantly unfussy country wine, it's a good companion to a hearty winter stew.   \n",
       "...                                                                                                                                                                                                                                                           ...   \n",
       "118835  Notes of honeysuckle and cantaloupe sweeten this deliciously feather-light spätlese. It's intensely juicy, quenching the palate with streams of tart tangerine and grapefruit acidity, yet wraps up with a kiss of honey and peach.                         \n",
       "118836  Citation is given as much as a decade of bottle age prior to release, which means it is pre-cellared and drinking at its peak. Baked cherry, cocoa and coconut flavors combine gracefully, with soft, secondary fruit compote highlights.                   \n",
       "118837  Well-drained gravel soil gives this wine its crisp and dry character. It is ripe and fruity, although the spice is subdued in favor of a more serious structure. This is a wine to age for a couple of years, so drink from 2017.                           \n",
       "118838  A dry style of Pinot Gris, this is crisp with some acidity. It also has weight and a solid, powerful core of spice and baked apple flavors. With its structure still developing, the wine needs to age. Drink from 2015.                                    \n",
       "118839  Big, rich and off-dry, this is powered by intense spiciness and rounded texture. Lychees dominate the fruit profile, giving an opulent feel to the aftertaste. Drink now.                                                                                   \n",
       "\n",
       "         index  \n",
       "0       0       \n",
       "1       1       \n",
       "2       2       \n",
       "3       3       \n",
       "4       4       \n",
       "...    ..       \n",
       "118835  118835  \n",
       "118836  118836  \n",
       "118837  118837  \n",
       "118838  118838  \n",
       "118839  118839  \n",
       "\n",
       "[118840 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines['index'] = wines.index\n",
    "wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_wines = wines['description'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500    [aromas, watermelon, dust, natural, vanilla, mark, bouquet, palate, fleshy, crisp, focus, nectarine, apple, strawberry, flavor, finish, last, sweetness]                                                                    \n",
       "501    [verdelho, taste, like, marcona, almonds, fruity, aromatic, good, swirl, mark, tangy, sweetness, finish]                                                                                                                    \n",
       "502    [deliciously, perfume, light, feather, bake, apple, lemon, flavor, crisp, honey, sweet, great, apéritif]                                                                                                                    \n",
       "503    [sauvignon, blanc, consistent, performer, hanna, usually, yield, rich, ripe, wine, brisk, acidity, best, efforts, tart, clean, savory, pineapple, tangerine, meyer, lemon, flavor, touch, slight, sweetness]                \n",
       "504    [simple, attractive, côtes, rhône, impressively, pure, cherry, berry, fruit, medium, body, slightly, chunky, feel, drink, couple, years]                                                                                    \n",
       "505    [smoky, bacon, blackberry, tire, rubber, nose, grate, feel, tannins, acids, wild, berry, flavor, bitter, black, plum, darkness, lengthy, finish]                                                                            \n",
       "506    [blend, pinot, grigio, ribolla, gialla, bright, white, open, floral, note, jasmine, honeysuckle, white, pepper, stone, fruit, ribolla, gialla, element, structure, note, caramel, pineapple]                                \n",
       "507    [medium, body, sharply, define, wine, bring, plenty, note, herb, root, earth, light, berry, fruit, capture, define, flavor, wahluke, slope, syrah, somewhat, mute, style, reflect, abbreviate, grow, season]                \n",
       "508    [fruit, warm, cool, vineyards, craft, clean, fresh, lightly, herbal, wine, please, dusty, character, hint, cocoa, toast, annotate, tart, berry, fruit, thread, smoke, chalk, tannins]                                       \n",
       "509    [straightforward, bordeaux, blend, petite, sirah, cabernet, sauvignon, merlot, cabernet, franc, malbec, source, howell, mountain, valley, floor, dusty, herbal, wine, sizably, tannic, dense, black, fruit, leather, coffee]\n",
       "510    [flint, smoke, aromas, strongly, bottle, alongside, hint, lemon, balm, rustic, apple, cider, vinegar, savory, kick, tangy, smoke, tangerine, flavor, energize, unique, sandalwood, smoky, quality, play, cider, like, tang] \n",
       "511    [potentially, rich, champagne, young, chardonnay, pinot, noir, dominate, blend, tight, acidity, minerality, tangy, orange, like, edge, time, mature, splendid]                                                              \n",
       "512    [extra, years, soften, tense, acidity, champagne, defiantly, bone, sear, ripe, rich, fruitiness, cushion, crisp, acidity, taut, nervy, citrus, character, drink]                                                            \n",
       "513    [tannins, support, layer, extract, concentrate, fruit, lean, cherry, cassis, currant, char, vanilla, speak]                                                                                                                 \n",
       "514    [bake, blueberry, boysenberry, aromas, ripe, delicate, merlot, saturate, deeply, tannic, flavor, coffee, chocolate, lemon, peel, precede, toasty, finish, overflow, chocolaty, linger, tannins, drink]                      \n",
       "515    [delicate, cranberry, strawberry, wine, highlight, uplift, acidity, plenty, length, breadth, medium, body, structure, please, streak, cardamom, consistent, accent, forest, floor, cola, figure]                            \n",
       "516    [relatively, dense, wine, grippy, tannin, youthful, desire, time, strength, show, beginnings, delicacy, grace, commingle, raspberry, cleave]                                                                                \n",
       "517    [soft, smooth, earthy, wine, unravel, youth, crisp, fresh, acidity, chewy, texture, offer, glorious, celebration, cherry, rhubarb, cleave]                                                                                  \n",
       "518    [wall, vineyard, local, bishop, wine, structure, firm, have, great, cherry, berry, fruit, number, premier, crus, producer, certainly, expressive, generous, fruit, balance, tannins, drink]                                 \n",
       "519    [deliciously, fruity, structure, body, wine, excellent, concentration, good, balance, effusive, aromas, blueberry, black, cherry, lead, opulent, overripe, berry, flavor, lightly, accent, cleave, cinnamon, nuances]       \n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_wines[500:520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dict = Dictionary(processed_wines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of documents</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>59857</td>\n",
       "      <td>flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>59119</td>\n",
       "      <td>wine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>50818</td>\n",
       "      <td>fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>36599</td>\n",
       "      <td>finish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>35607</td>\n",
       "      <td>aromas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>34178</td>\n",
       "      <td>palate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>30909</td>\n",
       "      <td>acidity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>30675</td>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>27464</td>\n",
       "      <td>tannins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>25120</td>\n",
       "      <td>cherry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>23327</td>\n",
       "      <td>ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>22428</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>21299</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>20454</td>\n",
       "      <td>spice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>16247</td>\n",
       "      <td>berry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number of documents     word\n",
       "35   59857                flavor \n",
       "31   59119                wine   \n",
       "9    50818                fruit  \n",
       "50   36599                finish \n",
       "3    35607                aromas \n",
       "14   34178                palate \n",
       "0    30909                acidity\n",
       "125  30675                drink  \n",
       "30   27464                tannins\n",
       "156  25120                cherry \n",
       "27   23327                ripe   \n",
       "154  22428                black  \n",
       "56   21299                note   \n",
       "114  20454                spice  \n",
       "18   16247                berry  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words_in_reviews = pd.DataFrame.from_dict(wine_dict.dfs, orient = 'index', columns = [\"number of documents\"])\n",
    "words_in_reviews.sort_values(\"number of documents\", ascending = False, inplace = True)\n",
    "\n",
    "list2 = words_in_reviews.index\n",
    "\n",
    "freq = {}\n",
    "for i in list2:\n",
    "    freq[wine_dict[i]] = words_in_reviews['number of documents'][i]\n",
    "\n",
    "words_in_reviews['word'] = freq.keys()\n",
    "\n",
    "words_in_reviews[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24351"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_wines = [wine_dict.doc2bow(review) for review in processed_wines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_wine = models.LdaMulticore(bow_wines, num_topics=10, id2word=wine_dict, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.047*\"flavor\" + 0.031*\"wine\" + 0.021*\"sweet\" + 0.019*\"pinot\" + 0.018*\"acidity\" + 0.015*\"like\" + 0.013*\"fruit\" + 0.013*\"drink\" + 0.011*\"good\" + 0.011*\"vanilla\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.083*\"wine\" + 0.048*\"fruit\" + 0.037*\"drink\" + 0.036*\"acidity\" + 0.028*\"ripe\" + 0.022*\"flavor\" + 0.017*\"tannins\" + 0.017*\"rich\" + 0.016*\"structure\" + 0.015*\"character\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.029*\"fruit\" + 0.029*\"flavor\" + 0.023*\"wine\" + 0.022*\"aromas\" + 0.021*\"black\" + 0.019*\"palate\" + 0.018*\"cherry\" + 0.016*\"spice\" + 0.015*\"nose\" + 0.013*\"finish\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.035*\"cherry\" + 0.031*\"palate\" + 0.031*\"aromas\" + 0.030*\"tannins\" + 0.027*\"black\" + 0.019*\"spice\" + 0.018*\"berry\" + 0.017*\"offer\" + 0.015*\"note\" + 0.015*\"drink\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.043*\"cabernet\" + 0.033*\"blend\" + 0.025*\"flavor\" + 0.024*\"sauvignon\" + 0.024*\"merlot\" + 0.023*\"blackberry\" + 0.019*\"wine\" + 0.017*\"chocolate\" + 0.016*\"cherry\" + 0.016*\"black\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.026*\"wine\" + 0.018*\"vineyard\" + 0.009*\"show\" + 0.009*\"fruit\" + 0.009*\"rise\" + 0.009*\"palate\" + 0.008*\"light\" + 0.008*\"cranberry\" + 0.007*\"bottle\" + 0.006*\"finish\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.042*\"flavor\" + 0.039*\"finish\" + 0.027*\"aromas\" + 0.026*\"green\" + 0.026*\"palate\" + 0.024*\"fruit\" + 0.016*\"citrus\" + 0.016*\"feel\" + 0.015*\"nose\" + 0.014*\"note\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.048*\"flavor\" + 0.040*\"finish\" + 0.036*\"aromas\" + 0.031*\"berry\" + 0.025*\"plum\" + 0.021*\"palate\" + 0.021*\"fruit\" + 0.017*\"herbal\" + 0.017*\"feel\" + 0.016*\"note\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.024*\"palate\" + 0.021*\"finish\" + 0.020*\"apple\" + 0.019*\"flavor\" + 0.018*\"white\" + 0.017*\"lemon\" + 0.017*\"aromas\" + 0.017*\"peach\" + 0.016*\"acidity\" + 0.016*\"fruit\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.035*\"wine\" + 0.028*\"fruit\" + 0.024*\"tannins\" + 0.020*\"black\" + 0.020*\"years\" + 0.017*\"flavor\" + 0.012*\"dark\" + 0.011*\"time\" + 0.011*\"blackberry\" + 0.011*\"structure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_wine.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_review(n):\n",
    "    print(wines.description[n])\n",
    "    print(\"\\n\")\n",
    "    print(processed_wines[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This opens with aromas of underbrush, leather, berry and a balsamic note. The forward palate offers dried cherry, white pepper, tobacco and the warmth of alcohol alongside firm tannins. Drink this sooner rather than later.\n",
      "\n",
      "\n",
      "['open', 'aromas', 'underbrush', 'leather', 'berry', 'balsamic', 'note', 'forward', 'palate', 'offer', 'cherry', 'white', 'pepper', 'tobacco', 'warmth', 'alcohol', 'alongside', 'firm', 'tannins', 'drink', 'sooner', 'later']\n"
     ]
    }
   ],
   "source": [
    "t2 = bow_wines[686]\n",
    "show_review(686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9608637094497681\t \n",
      "Topic: 0.035*\"cherry\" + 0.031*\"palate\" + 0.031*\"aromas\" + 0.030*\"tannins\" + 0.027*\"black\" + 0.019*\"spice\" + 0.018*\"berry\" + 0.017*\"offer\" + 0.015*\"note\" + 0.015*\"drink\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_wine[t2], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_wine.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
